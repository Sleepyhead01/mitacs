{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jk2YXdhhXs2p",
        "llrv8V8KW4Wv",
        "BvsrziqFXSD3",
        "p-APd48uXcO4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "jk2YXdhhXs2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtugfHFOMGYg",
        "outputId": "774b7abb-936c-4836-96b0-9c607b4bfa10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "_vpV6AZ8MNiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some experiments\n",
        "\n",
        "CodeT5 seems to be a pretty weak model, easily misled by wrong documentation. For comparison, I also tried to generate a summary using chatGPT for the same code. \n"
      ],
      "metadata": {
        "id": "ayjNkok2Uo-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expt. 1.1"
      ],
      "metadata": {
        "id": "DehTr8IDWvPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
        "text = \"\"\"\n",
        "def max_(a, b):\n",
        "  if a>b:\n",
        "    return a\n",
        "  else:\n",
        "    return b\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "generated_ids = model.generate(input_ids, max_length=20)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0zlnYGdMNZW",
        "outputId": "f0ee7f82-4824-4bfd-a18b-5aa0b5f68a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returns the maximal\n",
            " object in a and b.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT output \n"
      ],
      "metadata": {
        "id": "EMdzCOyfWqCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `max_` takes two arguments, `a` and `b`, and returns the larger of the two.\n",
        "\n",
        "Here's how the function works:\n",
        "\n",
        "1. It checks if `a` is greater than `b` using the `>` operator.\n",
        "\n",
        "2. If `a` is greater than `b`, the function returns `a`.\n",
        "\n",
        "3. If `a` is not greater than `b`, the function returns `b`.\n",
        "\n",
        "Here's an example usage of the `max_` function:\n",
        "\n",
        "```\n",
        ">>> max_(5, 10)\n",
        "10\n",
        ">>> max_(100, 50)\n",
        "100\n",
        ">>> max_(7, 7)\n",
        "7\n",
        "```\n",
        "\n",
        "In the first example, `10` is returned because it is greater than `5`. In the second example, `100` is returned because it is greater than `50`. In the third example, `7` is returned because `a` and `b` are equal."
      ],
      "metadata": {
        "id": "a5L6L4_1WkJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expt. 1.2"
      ],
      "metadata": {
        "id": "llrv8V8KW4Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "#returns maximum of two numbers\n",
        "def max_(a, b):\n",
        "  if a>b:\n",
        "    return a\n",
        "  else:\n",
        "    return b\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "generated_ids = model.generate(input_ids, max_length=20)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eQ7yeOhTE_V",
        "outputId": "bf03c4c4-8eee-4b03-a77e-156447758381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returns the maximum of two numbers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT output"
      ],
      "metadata": {
        "id": "uZicAsXHXCNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `max_()` takes two arguments `a` and `b`, and returns the maximum of the two numbers.\n",
        "\n",
        "Here's an explanation of how the function works:\n",
        "\n",
        "1. The function takes two arguments `a` and `b`.\n",
        "\n",
        "2. It compares `a` and `b` using the `>` operator.\n",
        "\n",
        "3. If `a` is greater than `b`, the function returns `a`.\n",
        "\n",
        "4. If `b` is greater than or equal to `a`, the function returns `b`.\n",
        "\n",
        "Here's an example of how to use the `max_()` function:\n",
        "\n",
        "```\n",
        ">>> max_(3, 5)\n",
        "5\n",
        ">>> max_(7, 2)\n",
        "7\n",
        ">>> max_(4, 4)\n",
        "4\n",
        "```\n",
        "\n",
        "In the first example, `a` is 3 and `b` is 5. Since `5` is greater than `3`, the function returns `5`.\n",
        "\n",
        "In the second example, `a` is 7 and `b` is 2. Since `7` is greater than `2`, the function returns `7`.\n",
        "\n",
        "In the third example, `a` is 4 and `b` is 4. Since `4` is equal to `4`, the function returns `4`."
      ],
      "metadata": {
        "id": "9TbV8gpFW82A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expt 1.3"
      ],
      "metadata": {
        "id": "BvsrziqFXSD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "#returns the time and date\n",
        "def max_(a, b):\n",
        "  if a>b:\n",
        "    return a\n",
        "  else:\n",
        "    return b\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "generated_ids = model.generate(input_ids, max_length=20)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw3aeivRTVUl",
        "outputId": "f7a23d08-b7d4-45b3-dedc-bbdf5fc4ad7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returns the maximum nanoseconds in a and b.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT output"
      ],
      "metadata": {
        "id": "SW4raTxTXOuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code defines a function called `max_` that takes in two arguments `a` and `b`. The function checks if `a` is greater than `b` using an `if` statement. If `a` is greater than `b`, it returns `a`. Otherwise, it returns `b`. \n",
        "\n",
        "This code does not return the time and date. It simply returns the larger of two given values."
      ],
      "metadata": {
        "id": "9jPWh4s1XGQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expt 1.4"
      ],
      "metadata": {
        "id": "p-APd48uXcO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "#returns the sum of two numbers\n",
        "def max_(a, b):\n",
        "  if a>b:\n",
        "    return a\n",
        "  else:\n",
        "    return b\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "generated_ids = model.generate(input_ids, max_length=20)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS98IzYDTclv",
        "outputId": "9358902b-51e9-4fb3-eadb-b6857e1882e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returns the sum of two numbers in the\n",
            "   .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT output"
      ],
      "metadata": {
        "id": "MH4ob9DaXMDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like this code defines a function called `max_` that takes two arguments, `a` and `b`. The function then compares `a` and `b` and returns the larger of the two numbers.\n",
        "\n",
        "However, the function name `max_` might be a bit misleading, as it implies that the function is returning the maximum value of two numbers. If you wanted to compute the maximum value of two numbers, you could modify the function like this:\n",
        "\n",
        "```\n",
        "def max_(a, b):\n",
        "    return max(a, b)\n",
        "```\n",
        "\n",
        "This would use the built-in `max` function to return the maximum value of `a` and `b`."
      ],
      "metadata": {
        "id": "6ESBaImZXJ_g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwSdrv-5TcjW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}